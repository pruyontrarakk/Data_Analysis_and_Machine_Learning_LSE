---
title: "ME314 2023"
output:
  html_document: default
  pdf_document: default
---

\newpage

## Question 1 -- **London Cycling Safety**

For this question, you will use data on 21492 cycling-involved incidents from 2017 to 2020 in London. These data are stored in the `cycling_severity.csv` file. Your goal is to use this data to build a model to predict the severity of traffic accidents. The data contains the following variables

| Variable          | Description|
|:------------------|:----------------------------------------------------|
|`severity_numeric`| A measure of the severity of the incident, ranging from 1 (Not serious) to 10 (Very serious)|
|`severity_binary`| A binary measure of severity (`"Not Severe"` or `"Severe"`)|
| `date`    | Date of the incident|
| `weekday`     | Day of the incident|
| `daytime`     | Time of day of the incident|
| `season` | Season of the incident|
| `weather_conditions`     | Weather at time of incident|
| `light_conditions`     | Light conditions at time of incident|
|`road_surface_conditions`| Road surface conditions at time of incident|
|`road_type`| Type of road on which incident occurred|
|`speed_limit`| Speed limit on road|
|`number_of_vehicles`| Number of vehicles involved in incident|
|`urban_or_rural_area`| Did the incident take place in a rural or an urban area?|
|`IMD_Decile`|Index of Multiple Deprivation Decile of area in which incident occurred. (1 means the most deprived and 10 represents the least deprived).|
|`IncScore`|Income Score (rate) of area in which incident occurred.|
|`EmpScore`|Employment Score (rate) of area in which incident occurred.|
|`HDDScore`|Health Deprivation and Disability Score of area in which incident occurred.|
|`EduScore`|Education, Skills and Training Score of area in which incident occurred.|
|`CriScore`|Crime Score of area in which incident occurred.|
|`EnvScore`|Living Environment Score of area in which incident occurred.|
: Variables in the `cycling_severity.csv` data.

Once you have downloaded this file and stored it somewhere sensible, you can load it into R using the following command:

```{r, echo = TRUE, eval = TRUE}

cycling <- read.csv("cycling_severity.csv")[,-1]

```

Your task is to apply at least one of the prediction or classification methods that we covered during the course to this data. You can choose to build a model for predicting either the `severity_numeric` variable (you can treat this as a continuous variable for the purposes of this question) or for the `severity_binary` variable. You can select any model we have covered on the course for this purpose. For instance, you might use a linear regression model, a logistic regression, a random forest model, a ridge regression, and so on. 


**Library Setup**
```{r, message = FALSE}

library(ggplot2)
library(randomForest)
library(stm)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(boot)
library(pROC)

```

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE) 
```

You will be awarded marks for:

1. Applying your chosen method (15 marks):

    - You should think carefully about which method to apply; which features to include; whether and how to include non-linear relationships; how to select any hyper-parameters of your model; and so on. Although simple models are often powerful, you are unlikely to receive high marks here for implementing a trivially simple model (such as a linear regression with a single explanatory variable).
    

**The method I have chosen to apply is the Random Forest Method. This method not only deals with non-linearlity but it chooses itself which features are the most important in providing insightful connections and analysis (which is why it is best to put in all the variables into the model aside from severity_numeric). For the hyper-parameter of mtry, I used general rule which is to take the square root of the number of variables in the model.)**
    
    
2. Demonstrating the predictive performance of your chosen method (15 marks). 

    - You might, for instance, calculate the MSE of your predictions for the quantitative response, or construct a confusion matrix and calculate accuracy/sensitivity/specificity for a qualitative response, etc. You will also need to think about the best data to use for evaluating the performance of your model (e.g. training data; train-test split; cross-validation; etc). 
    - You should provide a comparison of the predictive performance of your model to at least one alternative model specification.
    
**I compared my Random Forest Model to the Logistic Regression Model. Since all variables were used in random forest, I also used all the variables in logistic regression. However, I also tried finding only the statistically significant variables in the logistic regression but differences were minuscule.**

**I decided to split my data 70/30 because the training dataset should usually be larger than the testing so that it covers more points and minimizes variability and also because I had changed the values around and the changes were minuscule. I chose a threshold of whether to classify as severe or not severe for the random forest at 0.25 because looking at the threshold graph, the accuracy and specificity starts to flatten at about 0.25 while sensitivity is still decent.**

**I used the K-fold cross validation with 5 and 20 folds to run my logistic model and minimize potential variance. We can see that both k-folds estimates a test error rate of around 12% which is very similar to the training error rate.**


3. Interpreting the result of your method, commenting on how the results might be informative for people working to reduce the severity of cycling accidents (10 marks).

    - You may wish to report some measure of variable importance (plots of fitted values, tables of coefficients, plots of variable importance, etc).
    
**The varImpPlot determined some most important variables that affect the severity of cycing incidents: weather conditions, speed limit, environmental score, etc. Knowing these variables is informative for people working to reduce the severity of cycling accidents because they can make city-specific improvements or make people more aware. For further understanding, I created 2 box plots showing levels of weather conditions (important variable) and levels of urban/rural (not important variable). We can see that the proportions of severe incidents is nearly the same as non-severe incidents for weather conditions that were not "Fine". However, for the urban/rural, we can see that the proportion looks normal, like the "Fine" condition of the weather, showing it doesnt affect severity.**

**The Random Forest Modelhas 86.54% accuracy. It correctly identifies Severe cases 43.116% of the time (sensitivity) and correctly identifies Not Severe cases 92.936% of the time (specificity). The Logistic Regression Model has 88.82% accuracy. It was able to correctly identify positive cases 19.928% of the time and correctly identify negative cases 98.968% of the time. **

**The accuracy of both is high and similar, with low sensitivity but a high specificity. This means both models have high rate of correctly identifying Severe, but mis-classifies some Not Severe. Ideally, we would want to have balance between sensitivity/specificity while maintaining high accuracy. Random forest is best because it has a much higher sensitivity rate (specificity rate both are in the 90% range). ROC Curve (trade-off between sensitivity/specificity) also shows the Random Forest Model closer to (1,1) which means better sensitivity and specificity overall.**

4. Describing what advantages and/or disadvantages your chosen method has over alternative approaches (10 marks).

**The advantages of the Random Forest include being able to analyze both linearity and non linearity, being less prone to over fitting, being able to implicitly chose the best features itself for best accuracy/sensitivity/specificity, and being resistant to outliers. The disadvantages include being more difficult to understand more complex relationships in the data, being sometimes biased towards categorical variables with many levels, and being slower in the prediction phase due to dealing with the multiple trees.**

**In comparison, my logistic model ran significantly faster than the random forest in the prediction phase. However, due to the non linearity of the data, the model did not fit with as high of an accuracy and also not as high of sensitivity or specificity overall seen through the ROC model. You may also need to choose which predictors are the most statistically significant to use to generate the best model which is not required for random forest.**


Your answer should be no longer than 750 words.

**Make Factors**
```{r}

cycling1 <- cycling
char_vars <- sapply(cycling, is.character)
cycling[char_vars] <- lapply(cycling[char_vars], factor)


```


**Split Data Training and Testing**
```{r}

set.seed(3)
train <-  sample(nrow(cycling), 0.70 * nrow(cycling)) # Select 70% of the data
cycling_train <- cycling[train,]
cycling_test <- cycling[-train,]

```


**Give Appropiate Names**
```{r}

cycling1 <- cycling
cycling$daytime <- make.names(cycling$daytime)
cycling$weather_conditions <- make.names(cycling$weather_conditions)
cycling$light_conditions <- make.names(cycling$light_conditions)
cycling$road_type <- make.names(cycling$road_type)
cycling$severity_binary <- make.names(cycling$severity_binary)

```



**Random Forests Method**

```{r}

rf.cycling <-  randomForest(severity_binary ~ . -severity_numeric , 
                           data = cycling_train, 
                           mtry = 4)
yhat.bag <-  predict(rf.cycling, newdata = cycling_test, type = "prob")

```



**Threshold Selection Analysis**
```{r}

thresholds <- seq(0, 1, by = 0.01)
acc <- numeric(length(thresholds))
sen <- numeric(length(thresholds))
spec <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  pred <- ifelse(yhat.bag[,2] > thresholds[i], "Severe", "Not Severe")
  conmat <- confusionMatrix(as.factor(pred), cycling_test$severity_binary, positive = "Severe")
  
  acc[i] <- conmat$overall["Accuracy"]
  sen[i] <- conmat$byClass["Sensitivity"]
  spec[i] <- conmat$byClass["Specificity"]
}

plot_data <- data.frame(Threshold = thresholds, Accuracy = acc, Sensitivity = sen, Specificity = spec)

ggplot(plot_data, aes(x = Threshold)) +
  geom_line(aes(y = Accuracy, color = "Accuracy")) +
  geom_line(aes(y = Sensitivity, color = "Sensitivity")) +
  geom_line(aes(y = Specificity, color = "Specificity")) +
  labs(y = "Metric Value", color = "Metric") +
  theme_minimal()

```

**Random Forest Confusion Matrix**

```{r}
yhat.bag.pred <-  ifelse(yhat.bag[,2] > 0.25, "Severe", "Not Severe")
confusionMatrix(data = as.factor(yhat.bag.pred), reference = cycling_test$severity_binary, positive = "Severe")

```


**Important Variable Plot**
```{r}

varImpPlot(rf.cycling, cex = 1.2)

```



**Graph Further Show Which Weather Conditions Affect**
```{r}

barplot(table(cycling1[, c("severity_binary","weather_conditions")]),
        beside = TRUE,
        ylab = "Frequences",
        xlab = "Weather Conditions",
        main = "Weather Conditions and Severity Binary", 
        log = "y",
        col = c("seagreen3", "lightblue"))

legend("topright", legend = levels(cycling1$weather_conditions))
legend("center", legend = levels(cycling1$severity_binary), fill = c("seagreen3", "lightblue"))

```

**Graph Further Show Which Urban/Rural Affect**
```{r}

barplot(table(cycling1[, c("severity_binary","urban_or_rural_area")]),
        beside = TRUE,
        ylab = "Frequences",
        xlab = "Urban or Rural",
        main = "Urban or Rural and Severity Binary", 
        log = "y",
        col = c("seagreen3", "lightblue"))

legend("topleft", legend = levels(cycling1$severity_binary), fill = c("seagreen3", "lightblue"))

```


**Logistic Regression Method**
```{r}

glm.fit <-  glm(severity_binary ~ . -severity_numeric, data = cycling_train, family = binomial)

glm.probs <-  predict(glm.fit, newdata = cycling_test, 
                      type = "response")

glm.pred <-  rep("Not Severe", length(glm.probs))
glm.pred[glm.probs > 0.5] <-  "Severe"

confusionMatrix(data = as.factor(glm.pred), reference = cycling_test$severity_binary, positive = "Severe")

```



**K Fold for Logistic Regression 5 Fold**

```{r}
set.seed(1)
cost <- function(r, pi) mean(abs(r-pi)> 0.5)

cv_5_fold_logistic <-  cv.glm(cycling_train, glm.fit, K= 5, cost = cost)
cv_error_5_fold_logistic <- cv_5_fold_logistic$delta[1]
cv_error_5_fold_logistic

```


**K Fold for Logistic Regression 20 Fold**
```{r}
set.seed(2)
cv_20_fold_logistic <-  cv.glm(cycling_train, glm.fit, K= 20, cost = cost)
cv_error_20_fold_logistic <- cv_20_fold_logistic$delta[1]
cv_error_20_fold_logistic

```



**ROC Plot with Random Forest and Logistic**
```{r, message=FALSE}

#Random Forest
rf_pred_prob <- predict(rf.cycling, cycling_test, type = "prob")
roc_obj <- roc(as.factor(cycling_test$severity_binary), rf_pred_prob[,2])
plot(roc_obj, main = "ROC Curve", col = "green3", xlim = c(0, 1), ylim = c(0, 1))




#Logistic Regression
glm_pred_prob <- predict(glm.fit, cycling_test, type = "response")
roc_obj <- roc(as.factor(cycling_test$severity_binary), glm_pred_prob)
lines(roc_obj, main = "ROC Curve", col = "blue", xlim = c(0, 1), ylim = c(0, 1))


legend("bottomright", legend = c("Logistic Regression", "Random Forest"),
       col = c("blue", "green3"), lwd = 2)

```


\newpage

## Question 2 -- **NHS Patient Reviews** -- `nhs_reviews.Rdata`

For this question, you will use a set of 2000 patient reviews of NHS doctors' surgeries across the UK. The data contains the following variables:

| Variable          | Description|
|:------------------|:----------------------------------------------------|
| `review_title`    | The title of the patient's review|
| `review_text`     | The text of the patient's review |
| `star_rating`     | The star rating (out of five) that the patient gave|
| `review_positive` | A categorical indicator equal to `"Positive"` if the patient gave 3 stars or more in their review, and `"Negative"` if they gave 1 or 2 stars|
| `review_date`     | The date of the review|
| `gp_response`     | A categorical variable which measures whether the doctors' surgery provided a response to the patient's review (`"Responded"`) or has not yet provided a response (`"Has not responded"`) |
: Variables in the `nhs_reviews` data.

Once you have downloaded this file and stored it somewhere sensible, you can load it into R using the following command:

```{r, echo = TRUE, eval = TRUE}

load("nhs_reviews.Rdata")
nhs_reviews <- na.omit(nhs_reviews)
```

Your task is to apply at least one of the text analysis methods that we covered during the course to this data. Your goal in applying these methods is to generate insight for people who work within the NHS and would like to find ways to improve the service offered by GPs. You can select any text analysis method we covered on the course for this purpose. For instance, you might use a topic model, a dictionary-based approach, supervised classification, and so on.

You will be awarded marks for:

1. Applying your chosen method (15 marks). 

    - As with question 1, you should be ambitious here. You are unlikely to receive full credit for running only the very simplest analyses.
    
**I chose the Topic Model method. It allows me to understand which topics are most prevalent between the Positive and Negative reviews, which can be used when finding best ways to improve services offered by GP's.**
    
2. Discussing the feature-selection decisions you make and how they might affect the outcomes of the analysis (10 marks).

**The features that I removed were punctuation, numbers, symbols, stopwords, and the words “appointment” and “appointments”. By removing these features, you can create a cleaner and more focused representation of the reviews; this will help with identifying the best words for different topics. The words “appointment” and “appointments” were particularly removed because they showed up very often in both negative and positive reviews and thus carried little meaning. **

3. Providing some form of validation for your chosen method (15 marks).

**Looking at the validation graph that shows the diagnostic values by number of topics (K), higher held-out likelihood is preferred for model validation. This graph shows that having K = 5 would be best. It can also be seen in the semantic coherence that K=5 is also relatively high.**


4. Interpreting the output of the analysis, commenting on how the results might be informative to people working in the NHS (15 marks).

**After choosing 5 most relevant topics (from K = 5), I then chose the 4 that were statistically significant. By examining the coefficient of review_positive = Positive, we can see either it is a positive review (positive coefficient) or a negative review (negative coefficient). This is also confirmed by the estimate effect graphs.**

**There are 3 negative topics, and their titles are: "area_practices_years_office_catchment_registered_attitude_particular_worst_people" ,  "missed_told_joke_tomorrow_waited_said_hold_ringing_queue_e", and "impossible_pay_actually_use_get_it’s_mental_struggle_engaged_system". This information is informative to people working in the NHS because they can narrow in on 3 topics to improve on. For example, from the first title, maybe the employers can improve their attitude in particular catchment areas; from the second title, maybe the employers could work on cutting down wait and hold time on the phone; from the third title, maybe the institution could decrease workload with the same pay to prevent mental burdens on employees.**

**It is also important to review the positive topics as well to make sure that these qualities/actions found are maintained. The positive topic is: "arranged_within_examination_tests_dad_reassured_dr_cancer_follow_concerns". For example, maybe the employers were reassuring when providing patients with news about cancer, and that there was positive experience with examinations, tests, and follow ups.**


5. Critically assessing the strengths and weaknesses of your selected approach and proposing at least one alternative text analysis strategy that might be used in the selected application (10 marks).

**The strengths of topic modeling include being an unsupervised learning method which allows it to find topics from unstructured data on its own, being easily to interpret, and being able to discover topics along with its relevant words that might not be apparent. The weaknesses include being a bit difficult to choose the best number of topics and being ambiguous in its topics and words chosen (nuances and context of words can be lost).**

**One alternative text analysis strategy is dictionary modeling. With a dictionary, you have control over what topics you want to look at. This means that the words you find for each topic won’t be as ambiguous as some of the ones from the topic model could be. You also don’t worry about deciding the number of topics that would provide the best results. Although the dictionary method does have these positive qualities, it does have its own weaknesses which include being necessary to have a large pool of dictionary words to start off with and being limited by what words it can use in the dictionary. **


Your answer should be no longer than 750 words.



**Corpus and dfm**
```{r}

nhs_corpus <- nhs_reviews %>%
  corpus(text_field = "review_text")

nhs_dfm <- nhs_corpus %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
  dfm()

nhs_dfm <- nhs_dfm %>%
  dfm_remove(pattern = stopwords("english")) %>%
  dfm_remove(pattern = "appointment") %>%
  dfm_remove(pattern = "appointments")


```



**Make Diagnostic Values By Number of Topic**
```{r, results=FALSE}

set.seed(8)

search_stm_out <- searchK(documents = nhs_dfm,
  K = c(3,5,7,9,12),
  N = 300)

save(search_stm_out, file = "search_stm_out.Rdata")

```

**Plot Diagnostic Values By Number of Topic**
```{r}
plot(search_stm_out)
```


**Structural Topic Model**
```{r}

stm_out_prevalence <- stm(documents = nhs_dfm,
                          prevalence = ~review_positive,
                          K = 5,
                          seed = 12345,
                          verbose = FALSE)

save(stm_out_prevalence, file = "stm_out_prevalence.Rdata")

plot(stm_out_prevalence) #gives top words in plot

```




**Make Topic Labels**
```{r}
topic_labels <- apply(labelTopics(stm_out_prevalence, n = 10)$frex, 1, paste0, collapse = "_")
topic_labels

```

**Statistical Significance of Top K = 5 Topics**
```{r}
prevalence_effects <- estimateEffect(formula = c(1:5) ~ review_positive, 
                              stmobj = stm_out_prevalence,
                              metadata = docvars(nhs_dfm))

summary(prevalence_effects)

```

**Estimate Effect Graphs of Statistically Significant Topics**
```{r}
plot.estimateEffect(prevalence_effects,
     topics = 1,
     covariate = "review_positive",
     method = "pointestimate",
     main = topic_labels[1])

plot.estimateEffect(prevalence_effects,
     topics = 2,
     covariate = "review_positive",
     method = "pointestimate",
     main = topic_labels[2])

plot.estimateEffect(prevalence_effects,
     topics = 3,
     covariate = "review_positive",
     method = "pointestimate",
     main = topic_labels[3])

plot.estimateEffect(prevalence_effects,
     topics = 5,
     covariate = "review_positive",
     method = "pointestimate",
     main = topic_labels[5])




```



